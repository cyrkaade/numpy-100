{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+IS2GjWtGLbYEN37tPAZI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrkaade/numpy-100/blob/master/binaryclass2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa8g3U8c31dV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration & Hyperparameters\n",
        "# ==========================================\n",
        "DATA_DIR = 'dataset'  # Point this to your folder containing 'real' and 'fraud'\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "# Standard ResNet input size.\n",
        "# Note: For screen detection, higher res (e.g. 448) might capture pixel grids better\n",
        "# if 224 proves insufficient.\n",
        "INPUT_SIZE = 224\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Data Preparation\n",
        "# ==========================================\n",
        "\n",
        "# Define transformations\n",
        "# We normalize using ImageNet stats because we are using a pre-trained model\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((INPUT_SIZE, INPUT_SIZE)), # Resize to uniform size\n",
        "        transforms.RandomHorizontalFlip(),           # Augmentation\n",
        "        transforms.RandomRotation(10),               # Slight rotation\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "def get_data_loaders(data_dir):\n",
        "    # Load the full dataset\n",
        "    full_dataset = datasets.ImageFolder(data_dir)\n",
        "\n",
        "    # Calculate split sizes (80% train, 20% val)\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(0.8 * total_size)\n",
        "    val_size = total_size - train_size\n",
        "\n",
        "    # Split the dataset\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    # Apply the transforms manually to the subsets\n",
        "    # (Standard ImageFolder applies one transform to all, so we override specific indices here\n",
        "    # or simpler: wrap them in a helper class. For brevity, we assign transforms to the underlying dataset\n",
        "    # but since random_split doesn't copy, we strictly rely on the loader logic below or use this standard trick:)\n",
        "\n",
        "    # Reloading with specific transforms for clarity in this snippet:\n",
        "    train_set = datasets.ImageFolder(data_dir, transform=data_transforms['train'])\n",
        "    val_set = datasets.ImageFolder(data_dir, transform=data_transforms['val'])\n",
        "\n",
        "    # We use indices from the split to create subsets with correct transforms\n",
        "    train_subset = torch.utils.data.Subset(train_set, train_dataset.indices)\n",
        "    val_subset = torch.utils.data.Subset(val_set, val_dataset.indices)\n",
        "\n",
        "    dataloaders = {\n",
        "        'train': DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4),\n",
        "        'val': DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "    }\n",
        "\n",
        "    dataset_sizes = {'train': len(train_subset), 'val': len(val_subset)}\n",
        "    class_names = full_dataset.classes # Expect ['fraud', 'real'] or similar\n",
        "\n",
        "    return dataloaders, dataset_sizes, class_names\n",
        "\n",
        "# ==========================================\n",
        "# 3. Model Setup\n",
        "# ==========================================\n",
        "\n",
        "def initialize_model(num_classes):\n",
        "    # Load pre-trained ResNet18\n",
        "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    # Freeze initial layers (optional, but good for small datasets)\n",
        "    # for param in model.parameters():\n",
        "    #     param.requires_grad = False\n",
        "\n",
        "    # Replace the final fully connected layer\n",
        "    # ResNet18's last layer is called 'fc' and has 512 input features\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "# ==========================================\n",
        "# 4. Training Loop\n",
        "# ==========================================\n",
        "\n",
        "def train_model(model, criterion, optimizer, dataloaders, sizes, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                # Track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # Deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# 5. Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Check if data dir exists\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        print(f\"Error: Directory '{DATA_DIR}' not found. Please create it and add 'real' and 'fraud' subfolders.\")\n",
        "    else:\n",
        "        # Load Data\n",
        "        dataloaders, dataset_sizes, class_names = get_data_loaders(DATA_DIR)\n",
        "        print(f\"Classes found: {class_names}\")\n",
        "\n",
        "        # Initialize Model\n",
        "        model_ft = initialize_model(num_classes=len(class_names))\n",
        "\n",
        "        # Define Loss Function and Optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # SGD with momentum is a solid standard choice\n",
        "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "\n",
        "        # Train\n",
        "        model_ft = train_model(model_ft, criterion, optimizer_ft, dataloaders, dataset_sizes, num_epochs=NUM_EPOCHS)\n",
        "\n",
        "        # Save Model\n",
        "        torch.save(model_ft.state_dict(), 'barcode_fraud_detector.pth')\n",
        "        print(\"Model saved to barcode_fraud_detector.pth\")\n",
        "\n",
        "        # --- Quick Inference Test (Optional) ---\n",
        "        print(\"\\n--- Running Inference on a few validation images ---\")\n",
        "        model_ft.eval()\n",
        "        with torch.no_grad():\n",
        "            # Get one batch from validation\n",
        "            inputs, labels = next(iter(dataloaders['val']))\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model_ft(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for i in range(min(5, len(labels))):\n",
        "                print(f\"True: {class_names[labels[i]]} | Predicted: {class_names[preds[i]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERSION 2\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration\n",
        "# ==========================================\n",
        "DATA_DIR = 'dataset'\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 15\n",
        "LEARNING_RATE = 0.001\n",
        "INPUT_SIZE = 224\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Advanced Data Preparation (80/10/10)\n",
        "# ==========================================\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        # RandomResizedCrop helps focus on texture (moirÃ©/pixels)\n",
        "        transforms.RandomResizedCrop(INPUT_SIZE, scale=(0.7, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val_test': transforms.Compose([\n",
        "        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "def get_dataloaders(data_dir):\n",
        "    full_dataset = datasets.ImageFolder(data_dir)\n",
        "    n = len(full_dataset)\n",
        "\n",
        "    # Calculate indices for 80% train, 10% val, 10% test\n",
        "    indices = list(range(n))\n",
        "    np.random.shuffle(indices)\n",
        "    train_split = int(0.8 * n)\n",
        "    val_split = int(0.9 * n)\n",
        "\n",
        "    train_idx = indices[:train_split]\n",
        "    val_idx = indices[train_split:val_split]\n",
        "    test_idx = indices[val_split:]\n",
        "\n",
        "    # Create subsets with specific transforms\n",
        "    train_data = Subset(datasets.ImageFolder(data_dir, transform=data_transforms['train']), train_idx)\n",
        "    val_data = Subset(datasets.ImageFolder(data_dir, transform=data_transforms['val_test']), val_idx)\n",
        "    test_data = Subset(datasets.ImageFolder(data_dir, transform=data_transforms['val_test']), test_idx)\n",
        "\n",
        "    loaders = {\n",
        "        'train': DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2),\n",
        "        'val': DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2),\n",
        "        'test': DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "    }\n",
        "    return loaders, full_dataset.classes\n",
        "\n",
        "# ==========================================\n",
        "# 3. Model: EfficientNet-B0 (Better for Texture)\n",
        "# ==========================================\n",
        "def get_model(num_classes):\n",
        "    # Using EfficientNet_B0 for better feature extraction at 224px\n",
        "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    # Update the classifier head\n",
        "    num_ftrs = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "    return model.to(device)\n",
        "\n",
        "# ==========================================\n",
        "# 4. Training Engine\n",
        "# ==========================================\n",
        "def train_and_evaluate():\n",
        "    loaders, class_names = get_dataloaders(DATA_DIR)\n",
        "    model = get_model(len(class_names))\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Improvement: Scheduler helps the model settle into the best weights\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in loaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / len(loaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(loaders[phase].dataset)\n",
        "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        print(\"-\" * 15)\n",
        "\n",
        "    # ==========================================\n",
        "    # 5. Final Testing (The Unseen Data)\n",
        "    # ==========================================\n",
        "    print(\"\\nTraining Complete. Evaluating on Unseen TEST SET...\")\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loaders['test']:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Confusion Matrix Analysis\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(all_labels, all_preds))\n",
        "    print(\"\\nDetailed Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    torch.save(model.state_dict(), 'best_fraud_model.pth')\n",
        "    print(\"Model saved as best_fraud_model.pth\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_and_evaluate()"
      ],
      "metadata": {
        "id": "Zqhzt0GByI8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}